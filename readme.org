* TODO Mode d'emploi
** Installation

#+BEGIN_SRC bash
 pip install fast-eval
#+END_SRC
*** Optionnal requirements

**** For HTML export
Install pandoc

#+BEGIN_SRC bash
  sudo apt install pandoc
#+END_SRC

Install pandoc theme
#+BEGIN_SRC bash
  curl 'https://raw.githubusercontent.com/ryangrose/easy-pandoc-templates/master/copy_templates.sh' | bash
#+END_SRC

** Fichier de configuration
Champs Ã  adapter :

- required files :: Fichiers Ã  chercher dans le rendu des Ã©tudiants.

- reference folder :: Dossier dont le contenu est Ã  copier dans le
  dossier d'Ã©valuation de chaque rendu. Cela peut Ãªtre des /headers/
  nÃ©cessaires Ã  la compilation, des programmes de tests etc... ChaÃ®ne
  vide si pas besoin de dossier de rÃ©fÃ©rence.

- comp commands :: Liste de commandes Ã  effectuer lors de l'Ã©tape de
  compilation. Liste vide si rien Ã  faire.

- execution commands :: Liste de commandes Ã  effectuer lors de l'Ã©tape
  d'exÃ©cution/Ã©valuation. Liste vide si rien Ã  faire.
#+BEGIN_SRC json :tangle example/fake.json
  {
      "required_files": [
          "hello.c",
          "nohello.c"
      ],
      "reference_folder": "./ref",
      "compilation_commands": [
          "gcc hello.c -o hello -Wall",
          "gcc nohello.c -o nohello -Wall"
      ],
      "execution_commands": [
          "./hello",
          "./nohello"
      ]
  }
#+END_SRC

** Usage

#+BEGIN_SRC bash :results output :exports both
  fast-eval -h
#+END_SRC

#+RESULTS:
: usage: fast-eval [-h] [--ws WS] config archive_path
:
: positional arguments:
:   config        path of json config file
:   archive_path  path of archive from arche
:
: optional arguments:
:   -h, --help    show this help message and exit
:   --ws WS       where to build workspace

** Etapes de correction
1. ExÃ©cuter fast-eval
#+BEGIN_SRC bash :results output
  fast-eval example/fake.json example/fake.zip -v 2 -ws example/
#+END_SRC

#+RESULTS:
#+begin_example
Using  [38;5;37m[1m/home/virgile/ws/fast-eval/example[0m as workspace. [38;5;37m[1mâœ“[0m
Loaded [38;5;37m[1mexample/fake.json[0m savefile. [38;5;37m[1mâœ“[0m
Not using ref folder
Loaded [38;5;37m[1m/home/virgile/ws/fast-eval/example/data.json[0m savefile. [38;5;37m[1mâœ“[0m

Processing 3 projects...

Checking   1 projects...
           [48;5;131m[38;5;15m1 fails.[0m

Fail list : ['Dupond Vide']

Dupond Vide's errors :
 {'missing_files': ['hello.c', 'nohello.c']}


Wrote  [38;5;37m[1m/home/virgile/ws/fast-eval/example/data.json[0m savefile. [38;5;37m[1mâœ“[0m
Compiling  1 projects...
           [48;5;131m[38;5;15m1 fails.[0m

Fail list : ['DaugÃ© Virgile']

DaugÃ© Virgile's errors :
 {'gcc hello.c -o hello -Wall': {'stderr': ['hello.c: In function â€˜mainâ€™:', 'hello.c:1:12: warning: implicit declaration of function â€˜printfâ€™ [-Wimplicit-function-declaration]', '    1 | int main(){printf("coucou\\n"); oups = "1"; return 0;}', '      |            ^~~~~~', 'hello.c:1:12: warning: incompatible implicit declaration of built-in function â€˜printfâ€™', 'hello.c:1:1: note: include â€˜<stdio.h>â€™ or provide a declaration of â€˜printfâ€™', '  +++ |+#include <stdio.h>', '    1 | int main(){printf("coucou\\n"); oups = "1"; return 0;}', 'hello.c:1:32: error: â€˜oupsâ€™ undeclared (first use in this function)', '    1 | int main(){printf("coucou\\n"); oups = "1"; return 0;}', '      |                                ^~~~', 'hello.c:1:32: note: each undeclared identifier is reported only once for each function it appears in', '']}, 'gcc nohello.c -o nohello -Wall': {'stderr': ['nohello.c: In function â€˜mainâ€™:', 'nohello.c:1:20: warning: unused variable â€˜msgâ€™ [-Wunused-variable]', '    1 | int main(){ char * msg = "nohello"; return 0;}', '      |                    ^~~', '']}}


Wrote  [38;5;37m[1m/home/virgile/ws/fast-eval/example/data.json[0m savefile. [38;5;37m[1mâœ“[0m
Fail list : ['Zipeur Thomas']

Zipeur Thomas's errors :
 {}


Wrote  [38;5;37m[1m/home/virgile/ws/fast-eval/example/data.json[0m savefile. [38;5;37m[1mâœ“[0m
Wrote  [38;5;37m[1m/home/virgile/ws/fast-eval/example/readme.html[0m readable file. [38;5;37m[1mâœ“[0m
#+end_example

fast-eval a crÃ©e l'ensemble des Ã©lÃ©ments nÃ©cessaires Ã  la correction
dans le /workspace/ passÃ© en argument :

- Un dossier par Ã©tudiant
- Un fichier de sauvegarde /data.json/
- Un fichier readme /readme.org/ (Ã  ouvrir dans emacs)
- un fichier readme /readme.html/, plus lisible, contenant l'ensemble
  des donnÃ©es rÃ©coltÃ©es ainsi que le code fourni par l'Ã©tudiant.
#+BEGIN_SRC bash :results output
  tree example
#+END_SRC

#+RESULTS:
#+begin_example
example
â”œâ”€â”€ data.json
â”œâ”€â”€ DaugÃ© Virgile
â”‚Â Â  â”œâ”€â”€ eval
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ hello.c
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ nohello
â”‚Â Â  â”‚Â Â  â””â”€â”€ nohello.c
â”‚Â Â  â””â”€â”€ raw
â”‚Â Â      â”œâ”€â”€ a.out
â”‚Â Â      â”œâ”€â”€ fichierquitraine
â”‚Â Â      â”œâ”€â”€ hello
â”‚Â Â      â”œâ”€â”€ hello.c
â”‚Â Â      â””â”€â”€ nohello.c
â”œâ”€â”€ Dupond Vide
â”‚Â Â  â”œâ”€â”€ eval
â”‚Â Â  â””â”€â”€ raw
â”œâ”€â”€ fake.json
â”œâ”€â”€ fake.zip
â”œâ”€â”€ readme.html
â”œâ”€â”€ readme.org
â””â”€â”€ Zipeur Thomas
    â”œâ”€â”€ eval
    â”‚Â Â  â”œâ”€â”€ hello
    â”‚Â Â  â”œâ”€â”€ hello.c
    â”‚Â Â  â”œâ”€â”€ nohello
    â”‚Â Â  â””â”€â”€ nohello.c
    â””â”€â”€ raw
        â”œâ”€â”€ a.out
        â”œâ”€â”€ exo1fils.c
        â”œâ”€â”€ hello
        â”œâ”€â”€ hello.c
        â”œâ”€â”€ nohello.c
        â””â”€â”€ tommy.zip

9 directories, 23 files
#+end_example

2. RÃ©gler les soucis de prÃ©paration
Ici pas d'erreurs que prÃ©paration l'on puisse corriger. Souvent, il
s'agit d'un fichier ne respectant pas la convention de nommage imposÃ©e
par le sujet.  il faut donc copier *manuellement* les fichiers
incorrectment nommÃ©s depuis le dossier /raw/ de l'Ã©tudiant vers le
dossier /eval/ de l'Ã©tudiant, cette fois ci avec le bon nom.

example :
#+BEGIN_SRC bash
  mv example/Dupond\ Vide/raw/nom_incorrect.c example/Dupond\ Vide/eval/nom_correct.c
#+END_SRC

3. ExÃ©cuter fast-eval
#+BEGIN_SRC bash :results output
  fast-eval example/fake.json example/fake.zip -v 2 -ws example/
#+END_SRC

4. Consulter le rapport gÃ©nÃ©rÃ© pour correction
#+BEGIN_SRC bash :results output
  firefox example/readme.html
#+END_SRC

#+RESULTS:

** know Issues

Some Zip files unzip failed, idk why.

 - zip files not marked with .zip
 - other zip files
* Concept
** Pourquoi ?
L'objectif de ce projet est de faciliter l'Ã©valuation de TPs d'info.
GÃ©nÃ©ralement la procÃ©dure d'Ã©valuation est la mÃªme :

- RÃ©cupÃ©ration :: Je rÃ©cupÃ¨re tous les travaux soumis dans une unique
  archive fournie par Arche. (manuellement pour l'instant, il ne
  semble pas qu'il y ait d'API arche accessible).

- PrÃ©paration :: Chaque travail est gÃ©nÃ©ralement soumis sous la forme
  d'une archive, dont l'organisation varie souvent Ã©normÃ©ment d'un
  Ã©tudiant Ã  l'autre. Cette partie est donc fastidieuse : il faut
  extraire un Ã  un chaque archive, puis chercher les fichiers
  rÃ©ellement utiles (en gÃ©nÃ©ral un ou plusieurs fichiers source).

- Compilation :: Selon le projet et le langage, exÃ©cution de make,
  gcc etc... Idem, c'est fastidieux, et facilement scriptable.

- ExÃ©cution et Ã©valuation :: Faire tourner le programme et voir ce que
  cela donne. Une partie plus ou moins couvrante peut Ãªtre dÃ©lÃ©guÃ©e Ã 
  des logiciels de tests, permettant d'avoir rapidement une idÃ©e de la
  pertinence de la solution soumise.

** Comment ?

Automatisation de la prÃ©paration, compilation et pourquoi pas d'une
partie de l'Ã©valuation.

Cette automatisation ce concrÃ©tise par un programme python permettant
de faire une grosse partie du travail fastidieux et rÃ©pÃ©titif
nÃ©cessaire lors de l'Ã©valuation de TPs/projets.
* ImplÃ©mentation
** Package declaration
*** Fichier de setup
#+begin_src python :tangle setup.py
# -*- coding: utf-
from setuptools import setup, find_packages

setup(
    name='fast-eval',
    packages=find_packages(exclude=["examples/*"]),
    version='0.2.12',
    description='Simple tool to provide automation to assessment processes.',
    author=u'Virgile DaugÃ©',
    author_email='virgile.dauge@pm.me',
    url='https://github.com/Virgile-Dauge/fast-eval',
    # download_url='',
    keywords=['assessment', 'evaluation'],
    install_requires=['colored', 'alive-progress'],
    classifiers=[
        'Development Status :: 4 - Beta',
        'Environment :: Console',
        'Intended Audience :: Developers',
        'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
        'Operating System :: POSIX',
        'Programming Language :: Python :: 3.6',
        ],
    entry_points={
        'console_scripts': [
            'fast-eval=fast_eval.__main__:main',
        ],
    },
    python_requires='>=3.6',
)
#+end_src

#+BEGIN_SRC bash :results output :cache yes
mkdir fast_eval
tree .
#+END_SRC

#+RESULTS[5c942e2388023d571e100ded4100f76a38d146f2]:
: .
: â”œâ”€â”€ fast_eval
: â”‚Â Â  â”œâ”€â”€ fast_eval.py
: â”‚Â Â  â””â”€â”€ __main__.py
: â”œâ”€â”€ LICENSE
: â”œâ”€â”€ readme.org
: â””â”€â”€ setup.py
:
: 1 directory, 5 files

#+begin_src python :tangle fast_eval/__init__.py
#+end_src
** Cli

#+begin_src python :tangle fast_eval/__main__.py
    #!/usr/bin/env python3
    import argparse
    from fast_eval.util import FastEval
    def main():
        parser = argparse.ArgumentParser()
        parser.add_argument("config",
                            help="path of json config file")
        parser.add_argument("archive_path",
                            help="path of archive from arche")
        parser.add_argument("-ws", "--workspace",
                            help="where to build workspace")
        parser.add_argument("-v", "--verbosity",
                            help="increase output verbosity",
                            type=int, choices=[0, 1, 2], default=0)
        fe = FastEval(parser.parse_args())
#+end_src

#+RESULTS:

** DÃ©pendances

#+begin_src python :tangle fast_eval/util.py :noweb yes
  # Pour lecture de dossiers/fichiers
  import os
  import sys
  import csv
  import json
  import shlex
  # Pour affichage de dict
  import pprint
  # Pour dÃ©comprÃ©sser
  import shutil
  # Pour ExÃ©cution de programmes
  import subprocess
  from alive_progress import alive_bar

  from colored import fg, bg, attr
  # Helpers
  <<list_files>>
  <<search_files>>
  <<choice_str>>
#+end_src

** TODO Class
*** Init
   Initialization :
#+begin_src python :tangle fast_eval/util.py :noweb yes
  class FastEval:
      """
      @brief Simple tool to provide automation to assessment processes.
      @details Provide tools to build, compile and evaluatue a suitable
      workspace with a specific working folder for each submitted
      project from a single compressed archive.

      """
      def __init__(self, args):
          "docstring"
          self.ecolor = bg('indian_red_1a') + fg('white')
          #self.ecolor = fg('red_3a')
          #self.wcolor = bg('orange_1') + fg('white')
          self.wcolor = fg('orange_1')
          #self.icolor = bg('deep_sky_blue_2') + fg('white')
          #self.icolor = fg('medium_turquoise') + attr('bold')
          self.icolor = fg('light_sea_green') + attr('bold')
          self.rcolor = attr('reset')
          if args.workspace:
              self.workspace_path = os.path.abspath(os.path.expanduser(args.workspace))
          else:
              self.workspace_path = os.path.join(os.getcwd(), 'submissions')
          print(f'Using  {self.info_str(self.workspace_path)} as workspace. {self.info_str("âœ“")}')

          self.archive_path = os.path.expanduser(args.archive_path)
          if not os.path.exists(self.archive_path):
              print('Given  {}'
                    ' does not exist, exiting...'.format(self.erro_str(self.archive_path)),
                    file=sys.stderr)
              sys.exit()

          self.verbosity = args.verbosity
          config_path = os.path.expanduser(args.config)
          assert os.path.isfile(config_path), "{} is not a file.".format(self.erro_str(config_path))

          with open(config_path, 'r') as fp:
              config = json.load(fp)
          print(f'Loaded {self.info_str(config_path)} savefile. {self.info_str("âœ“")}')
          self.required_files = config['required_files']

          if len(config['reference_folder']) > 0:
              self.ref_path = os.path.expanduser(config['reference_folder'])
              if not os.path.isdir(self.ref_path):
                  print('Given  {}'
                    ' does not exist, exiting...'.format(self.erro_str(self.ref_path)),
                    file=sys.stderr)
                  sys.exit()
              print(f'Using  {self.info_str(self.ref_path)} as reference folder. {self.info_str("âœ“")}')
          else:
              self.ref_path = None
              print('Not using ref folder')

          if 'compilation_commands' in config:
              self.comp_cmd = config['compilation_commands']
          else:
              self.comp_cmd = []
          if 'execution_commands' in config:
              self.exec_cmd = config['execution_commands']
          else:
              self.exec_cmd = []
          if 'cleanup' in config:
              self.cleanup_cmd = config['cleanup']
          else:
              self.cleanup_cmd = []
          if 'export_to_html' in config:
              self.export_to_html = config['export_to_html']
          else:
              self.export_to_html = True

          self.submissions = {}
          # Chargement de la config
          self.load_data()
          # Si c'est le premier passage, il faut lancer la preparation
          if self.pass_count == 0:
              shutil.unpack_archive(self.archive_path, self.workspace_path)
              submissions = self.clean_dirs()
              print('Processing {} projects...\n'.format(len(submissions)))
              self.submissions = {key: dict(value, **{'step' : '0_prep', 'steps': {'0_prep' : {},
                                                                                   '1_comp' : {},
                                                                                   '2_exec' : {},
                                                                                   '3_eval' : {}}}) for key, value in submissions.items()}
              self.extract_dirs()
              self.copy_ref()
              print('\n')
              self.prep_step()
              self.gen_csv()
          else:
              print('Processing {} projects...\n'.format(len(self.submissions)))
              self.check_prep()

          self.print_step_errors('0_prep')
          self.write_data()
          self.exte_step(self.comp_cmd, step='1_comp', label='Compiling')
          self.print_step_errors('1_comp')
          self.write_data()
          self.exte_step(self.exec_cmd, step='2_exec', label='Executing')
          self.cleanup()
          self.print_step_errors('2_exec')
          self.write_data()
          self.export()

      <<load_data>>
      <<write_data>>
      <<clean_dirs>>
      <<extract_dirs>>
      <<copy_ref>>
      <<prep_step>>
      <<check_prep>>
      <<exte_step>>
      <<cleanup>>
      <<export>>
      <<gen_html>>
      <<gen_csv>>
      <<next_step>>
      <<erro_str>>
      <<warn_str>>
      <<info_str>>
      <<print_step_errors>>


#+end_src

*** Print Helpers
#+name: choice_str
#+begin_src python
  def choice_str(choices, target=''):
      res = '. ' + str(target) + '\n' + 'â”‚\n'
      for choice in choices[:-1]:
        res = res + 'â”œâ”€â”€ ' + str(choice) + '\n'
      res = res + 'â””â”€â”€ ' + choices[-1]
      return res
#+end_src

#+name: warn_str
#+begin_src python
  def warn_str(self, msg):
      return self.wcolor + str(msg) + self.rcolor
#+end_src

#+name: erro_str
#+begin_src python
  def erro_str(self, msg):
      return self.ecolor + str(msg) + self.rcolor
#+end_src

#+name: info_str
#+begin_src python
  def info_str(self, msg):
      return self.icolor + str(msg) + self.rcolor
#+end_src

#+name: print_step_errors
#+begin_src python
  def print_step_errors(self, step):
      to_print = [sub for sub in self.submissions if self.submissions[sub]['step'] == step]
      if self.verbosity >= 1 and len(to_print) > 0:
          print(f"Fail list : {to_print}\n")
      if self.verbosity > 1:
          for s in to_print:
              msg = f'{s}\'s errors : \n {self.submissions[s]["steps"][step]}'
              if len(msg) < 200:
                  print(msg)
      print("\n")
#+end_src

*** Json data files
#+name: load_data
#+begin_src python
  def load_data(self):
      data_file = os.path.join(self.workspace_path, 'data.json')
      #data = load_json(data_file)
      try:
          with open(data_file, 'r') as fp:
              data = json.load(fp)


          self.pass_count = data['pass_count'] + 1
          self.submissions = data['submissions']
          print(f'Loaded {self.info_str(data_file)} savefile. {self.info_str("âœ“")}\n')
      except FileNotFoundError:
          print(f'Using  {self.info_str(data_file)} savefile. {self.info_str("âœ“")}\n')
          self.pass_count = 0
#+end_src

#+name: write_data
#+begin_src python
    def write_data(self):
        data_file = os.path.join(self.workspace_path, 'data.json')
        try:
            with open(data_file, 'w') as fp:
                json.dump({'pass_count': self.pass_count,
                           'submissions': self.submissions},
                          fp, sort_keys=True, indent=4, ensure_ascii=False)
            print(f'Wrote  {self.info_str(data_file)} savefile. {self.info_str("âœ“")}')
        except:
            print('Error while writing : \n => {}\n'.format(data_file),
                  file=sys.stderr)

#+end_src
*** PrÃ©paration
#+name: clean_dirs
#+begin_src python
  def clean_dirs(self):
      submissions = {o[:-32]:{"path": os.path.join(self.workspace_path, o)} for o in os.listdir(self.workspace_path)
                     if os.path.isdir(os.path.join(self.workspace_path, o))}
      for sub in submissions.values():
          if not os.path.exists(sub["path"][:-32]):
              shutil.move(sub['path'], sub['path'][:-32])
          if 'assignsubmission_file' in sub ['path']:
              sub['path'] = sub['path'][:-32]
      return submissions
#+end_src

#+name: extract_dirs
#+begin_src python
  def extract_dirs(self):
      for sub in self.submissions:
          raw_dir = os.path.join(self.submissions[sub]['path'], 'raw')
          os.mkdir(raw_dir)
          for o in os.listdir(self.submissions[sub]['path']):
              shutil.move(os.path.join(self.submissions[sub]['path'],o), raw_dir)
          files = [os.path.join(raw_dir, f) for root, _, files in os.walk(raw_dir) for f in files]
          print(files)
          for f in files:
              try:
                  shutil.unpack_archive(f, raw_dir)
                  #os.remove(f)
              except shutil.ReadError:
                  print('Unpack ' + self.warn_str(f) + ' failed.')

#+end_src

#+name: copy_ref
#+begin_src python
  def copy_ref(self):
      if self.ref_path is not None:
          for sub in self.submissions:
              shutil.copytree(self.ref_path, os.path.join(self.submissions[sub]['path'], 'eval'))

#+end_src

#+name: prep_step
#+begin_src python
  def prep_step(self):
      to_prep = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
      print('Preparing  {} projects...'.format(len(to_prep)))

      with alive_bar(len(to_prep)) as bar:
          for sub in to_prep:
              raw_dir = os.path.join(self.submissions[sub]['path'], 'raw')
              eval_dir = os.path.join(self.submissions[sub]['path'], 'eval')

              if not os.path.exists(eval_dir):
                  os.mkdir(eval_dir)

              missing_files = []

              # Search every required files one by one
              for f in self.required_files:
                  # List cadidates for searched file
                  student_code = search_files(f, raw_dir)
                  # Filter files in a "__MACOS" directory
                  student_code = [s for s in student_code if '__MACOS' not in s]
                  if len(student_code) == 1:
                      shutil.copyfile(student_code[0], os.path.join(eval_dir, f))
                  elif len(student_code) == 0:
                      missing_files.append(f)
                  else:
                      msg = 'You need to manually copy one of those files'
                      msg = msg + choice_str(student_code, f)
                      self.submissions[sub]['steps']['0_prep']['msg'] = msg

              # Update missing files if needed
              if missing_files:
                  if 'missing_files' not in self.submissions[sub]['steps']['0_prep']:
                      self.submissions[sub]['steps']['0_prep']['missing_files'] = missing_files
                  else:
                      self.submissions[sub]['steps']['0_prep']['missing_files'].extend(missing_files)
              else:
                  self.submissions[sub]['step'] = '1_comp'

              bar()

      to_prep = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
      if len(to_prep) == 0:
          print(f'           0 fails. {self.info_str("âœ“")}')
      else:
          print('           ' + self.erro_str('{} fails.'.format(len(to_prep))) + '\n')
#+end_src
#+name: search_files
#+begin_src python
  def search_files(name, d='.'):
      return [os.path.join(root, f) for root, _, files in os.walk(d) for f in files if f == name]
#+end_src

#+name: check_prep
#+begin_src python
  def check_prep(self):
      to_check = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
      print(f'Checking   {len(to_check)} projects...')
      with alive_bar(len(to_check)) as bar:
          for sub in to_check:
              eval_dir = os.path.join(self.submissions[sub]['path'], 'eval')
              eval_files = [f for root, dirs, files in os.walk(eval_dir) for f in files]


              missing_files = [f for f in self.required_files if f not in eval_files]
              # Update missing files if needed
              if missing_files:
                  self.submissions[sub]['steps']['0_prep']['missing_files'] = missing_files
              else:
                  self.submissions[sub]['step'] = '1_comp'

              to_check = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
              bar()
          if len(to_check) == 0:
              print(f'0 fails. {self.info_str("âœ“")}')
          else:
              print(self.erro_str(f'{len(to_check)} fails.') + '\n')
#+end_src
*** Compilation

#+name: next_step
#+begin_src python
  def next_step(self, step):
      if step == '0_prep':
          return '1_comp'
      elif step == '1_comp':
          return '2_exec'
      elif step == '2_exec':
          return '3_eval'
      else:
          return 'done'
#+end_src

#+name: exte_step
#+begin_src python
  def exte_step(self, cmd, step='1_comp', label='Compiling', timeout=10):
      to_exec = [sub for sub in self.submissions if self.submissions[sub]['step'] == step]
      print('{}  {} projects...'.format(label, len(to_exec)))
      if not cmd:
          print('Nothing to do.')
          return 0
      root_dir = os.getcwd()
      with alive_bar(len(to_exec)) as bar:
          for sub in to_exec:
              os.chdir(os.path.join(self.submissions[sub]['path'], 'eval'))
              comp_ok = True
              timeout_raised = False
              for c in cmd:
                  try:
                      completed_process = subprocess.run([c], capture_output=True, text=True, shell=True, timeout=timeout)
                      if completed_process.returncode != 0:
                          comp_ok=False
                          cond = [len(completed_process.stderr) > 0, len(completed_process.stdout)]
                      if any(cond) and c not in self.submissions[sub]['steps'][step]:
                          self.submissions[sub]['steps'][step][c] = {}
                      if cond[0]:
                          self.submissions[sub]['steps'][step][c]['stderr'] = completed_process.stderr.split('\n')
                      if cond[1]:
                          out = completed_process.stdout.split('\n')
                          if len(out) > 20:
                              out = out[:10] + ['.'] + ['truncated by fast-eval'] + ['.'] + out[-10:]
                          self.submissions[sub]['steps'][step][c]['stdout'] = out
                  except Exception as e:
                      comp_ok=False
                      if type(e) is subprocess.TimeoutExpired:
                          self.submissions[sub]['steps'][step][c] = 'timeout'

              if comp_ok:
                  self.submissions[sub]['step'] = self.next_step(step)
              bar()
      os.chdir(root_dir)
      to_exec = [sub for sub in self.submissions if self.submissions[sub]['step'] == step]
      if len(to_exec) == 0:
          print(f' 0 fails. {self.info_str("âœ“")}')
      else:
          print(' ' + self.erro_str('{} fails.'.format(len(to_exec))) + '\n')

#+end_src

*** Cleanup

#+name: cleanup
#+begin_src python
  def cleanup(self):
      for c in self.cleanup_cmd:
          completed_process = subprocess.run(shlex.split(c))
          if completed_process.returncode == 0:
              print(f'Cleanup : {c} {self.info_str("âœ“")}')
          else:
              print(f'Cleanup : {c} {self.erro_str("âŒ")}')
#+end_src
*** Export vers org-mode

#+name: export
#+begin_src python
  def export(self):
      outpath = os.path.join(self.workspace_path, 'readme.org')
      with open(outpath, 'w') as f:
          f.write("#+title: Rapport d'Ã©valuation\n")
          for s in self.submissions:
              step = self.submissions[s]['step']
              steps = self.submissions[s]['steps']
              f.write(f'** {s}\n')

              # Section erreur prep
              if steps['0_prep']:
                  f.write(f'*** Erreurs de prÃ©paration\n')
                  for k, v in steps['0_prep'].items():
                      f.write(f'{k} :\n')
                      for i in v:
                          f.write(f' - {i}\n')
              # Section erreur comp
              if steps['1_comp']:
                  usefull = False
                  for v in steps['1_comp'].values():
                      if 'stderr' in v and v['stderr'] and len(v['stderr'][0])>0:
                          usefull = True
                  if usefull:
                      f.write(f'*** Erreurs de compilation\n')
                      for k, v in steps['1_comp'].items():
                          f.write(f'#+begin_src bash\n')
                          f.write(f'{k}\n')
                          f.write('#+end_src\n')

                          f.write('\n#+name: stderror\n')
                          f.write(f'#+begin_example\n')
                          for line in v['stderr']:
                              f.write(f'{line}\n')
                          f.write('\n#+end_example\n')

              # Section avec code rendu
              if step != '0_prep':
                  f.write(f'*** code\n')
                  for sf in self.required_files:
                      f.write(f'**** {sf}\n')
                      # DÃ©termination du langage
                      l = os.path.splitext(sf)[-1][1:]
                      if l == 'py':
                          l = python
                      if l == 'sh':
                          l = bash
                      # Copie du code de l'Ã©tudiant
                      f.write(f'#+begin_src {l}\n')
                      with open(os.path.join(self.submissions[s]['path'], 'eval', sf), 'r') as cf:
                          f.write(cf.read())
                      f.write('\n#+end_src\n')

              # Section retour exÃ©cution
              if steps['2_exec']:
                  f.write(f"*** Retours d'Ã©xÃ©cution\n")
                  for k, v in steps['2_exec'].items():
                      f.write(f'#+begin_src bash\n')
                      f.write(f'{k}\n')
                      f.write('#+end_src\n')
                  if 'stderr' in v:
                      f.write('\n#+name: stderror\n')
                      f.write(f'#+begin_example\n')
                      for line in v['stderr']:
                          f.write(f'{line}\n')
                      f.write('#+end_example\n')
                  if 'stdout' in v:
                      f.write('\n#+name: stdout\n')
                      f.write(f'#+begin_example\n')
                      for line in v['stdout']:
                          f.write(f'{line}\n')
                      f.write('#+end_example\n')
      if self.export_to_html:
          self.gen_html()
#+end_src
*** org vers html
#+name: gen_html
#+begin_src python
  def gen_html(self, orgfile='readme.org', style='tango'):
      inpath = os.path.join(self.workspace_path, 'readme.org')
      outpath = os.path.join(self.workspace_path, 'readme.html')
      cmd = shlex.split(f'pandoc -s {inpath} -o {outpath} --highlight-style {style} --template=easy_template.html --standalone --toc')
      completed_process = subprocess.run(cmd)
      if completed_process.returncode == 0:
          print(f'Wrote  {self.info_str(outpath)} readable file. {self.info_str("âœ“")}')
      else:
         print('Error while generating html')

#+end_src
*** gen csv with names
#+name: gen_csv
#+begin_src python
  def gen_csv(self):
      outpath = os.path.join(self.workspace_path, 'notes.csv')
      with open(outpath, 'w') as f:
          names = [s for s in self.submissions]
          names.sort()
          print(names)
          for n in names:
              f.write(f'{n}, note\n')
#+end_src
* DÃ©ploiement
** Vers Pypi

#+BEGIN_SRC bash :results output
rm -rf dist/
python setup.py sdist
#+END_SRC

#+RESULTS[8b5455aa48284574821df6568e30b543c07617d9]:
#+begin_example
running sdist
running egg_info
writing fast_eval.egg-info/PKG-INFO
writing dependency_links to fast_eval.egg-info/dependency_links.txt
writing entry points to fast_eval.egg-info/entry_points.txt
writing requirements to fast_eval.egg-info/requires.txt
writing top-level names to fast_eval.egg-info/top_level.txt
reading manifest file 'fast_eval.egg-info/SOURCES.txt'
writing manifest file 'fast_eval.egg-info/SOURCES.txt'
running check
creating fast-eval-0.2.12
creating fast-eval-0.2.12/fast_eval
creating fast-eval-0.2.12/fast_eval.egg-info
copying files to fast-eval-0.2.12...
copying setup.py -> fast-eval-0.2.12
copying fast_eval/__init__.py -> fast-eval-0.2.12/fast_eval
copying fast_eval/__main__.py -> fast-eval-0.2.12/fast_eval
copying fast_eval/util.py -> fast-eval-0.2.12/fast_eval
copying fast_eval.egg-info/PKG-INFO -> fast-eval-0.2.12/fast_eval.egg-info
copying fast_eval.egg-info/SOURCES.txt -> fast-eval-0.2.12/fast_eval.egg-info
copying fast_eval.egg-info/dependency_links.txt -> fast-eval-0.2.12/fast_eval.egg-info
copying fast_eval.egg-info/entry_points.txt -> fast-eval-0.2.12/fast_eval.egg-info
copying fast_eval.egg-info/requires.txt -> fast-eval-0.2.12/fast_eval.egg-info
copying fast_eval.egg-info/top_level.txt -> fast-eval-0.2.12/fast_eval.egg-info
Writing fast-eval-0.2.12/setup.cfg
creating dist
Creating tar archive
removing 'fast-eval-0.2.12' (and everything under it)
#+end_example

#+BEGIN_SRC bash
  twine upload dist/*
#+END_SRC

#+RESULTS:
** Github Pages

#+BEGIN_SRC bash
mkdir docs
#+END_SRC

#+BEGIN_SRC yaml :tangle docs/_config.yml
theme: jekyll-theme-architect
#+END_SRC

#+BEGIN_SRC bash
cp readme.md docs/index.md
#+END_SRC

#+RESULTS:
