* TODO Mode d'emploi
** Installation

#+BEGIN_SRC bash
 pip install fast-eval
#+END_SRC

** Fichier de configuration
Champs à adapter :

- required_files :: Fichiers à chercher dans le rendu des étudiants.

- reference_folder :: Dossier dont le contenu est à copier dans le
  dossier d'évaluation de chaque rendu. Cela peut être des /headers/
  nécessaires à la compilation, des programmes de tests etc... Chaîne
  vide si pas besoin de dossier de référence.

- comp_commands :: Liste de commandes à effectuer lors de l'étape de
  compilation. Liste vide si rien à faire.

- execution_commands :: Liste de commandes à effectuer lors de l'étape
  d'exécution/évaluation. Liste vide si rien à faire.
#+BEGIN_SRC json :tangle example/fake.json
{
    "required_files": [
        "hello.c",
        "nohello.c"
    ],
    "reference_folder": "~/coucou_ref",
    "compilation_commands": [
        "gcc hello.c -o hello -Wall",
        "gcc nohello.c -o nohello -Wall"
    ],
    "execution_commands": [
        "./hello",
 	"./nohello"
    ]
}
#+END_SRC

** Usage

#+BEGIN_SRC bash :results output :exports both
  fast-eval -h
#+END_SRC

#+RESULTS:
: usage: fast-eval [-h] [--ws WS] config archive_path
:
: positional arguments:
:   config        path of json config file
:   archive_path  path of archive from arche
:
: optional arguments:
:   -h, --help    show this help message and exit
:   --ws WS       where to build workspace

* Concept
** Pourquoi ?
L'objectif de ce projet est de faciliter l'évaluation de TPs d'info.
Généralement la procédure d'évaluation est la même :

- Récupération :: Je récupère tous les travaux soumis dans une unique
  archive fournie par Arche. (manuellement pour l'instant, il ne
  semble pas qu'il y ait d'API arche accessible).

- Préparation :: Chaque travail est généralement soumis sous la forme
  d'une archive, dont l'organisation varie souvent énormément d'un
  étudiant à l'autre. Cette partie est donc fastidieuse : il faut
  extraire un à un chaque archive, puis chercher les fichiers
  réellement utiles (en général un ou plusieurs fichiers source).

- Compilation :: Selon le projet et le langage, exécution de make,
  gcc etc... Idem, c'est fastidieux, et facilement scriptable.

- Exécution et évaluation :: Faire tourner le programme et voir ce que
  cela donne. Une partie plus ou moins couvrante peut être déléguée à
  des logiciels de tests, permettant d'avoir rapidement une idée de la
  pertinence de la solution soumise.

** Comment ?

Automatisation de la préparation, compilation et pourquoi pas d'une
partie de l'évaluation.

Cette automatisation ce concrétise par un programme python permettant
de faire une grosse partie du travail fastidieux et répétitif
nécessaire lors de l'évaluation de TPs/projets.

* Implémentation
** Package declaration
*** Fichier de setup
#+begin_src python :tangle setup.py
# -*- coding: utf-
from setuptools import setup, find_packages

setup(
    name='fast-eval',
    packages=find_packages(exclude=["examples/*"]),
    version='0.2.7',
    description='Simple tool to provide automation to assessment processes.',
    author=u'Virgile Daugé',
    author_email='virgile.dauge@pm.me',
    url='https://github.com/Virgile-Dauge/fast-eval',
    # download_url='',
    keywords=['assessment', 'evaluation'],
    install_requires=['colored'],
    classifiers=[
        'Development Status :: 4 - Beta',
        'Environment :: Console',
        'Intended Audience :: Developers',
        'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
        'Operating System :: POSIX',
        'Programming Language :: Python :: 3.6',
        ],
    entry_points={
        'console_scripts': [
            'fast-eval=fast_eval.__main__:main',
        ],
    },
    python_requires='>=3.6',
)
#+end_src

#+BEGIN_SRC bash :results output :cache yes
mkdir fast_eval
tree .
#+END_SRC

#+RESULTS[5c942e2388023d571e100ded4100f76a38d146f2]:
: .
: ├── fast_eval
: │   ├── fast_eval.py
: │   └── __main__.py
: ├── LICENSE
: ├── readme.org
: └── setup.py
:
: 1 directory, 5 files

#+begin_src python :tangle fast_eval/__init__.py
#+end_src
** Cli

#+begin_src python :tangle fast_eval/__main__.py
    #!/usr/bin/env python3
    import argparse
    from fast_eval.util import FastEval
    def main():
        parser = argparse.ArgumentParser()
        parser.add_argument("config",
                            help="path of json config file")
        parser.add_argument("archive_path",
                            help="path of archive from arche")
        parser.add_argument("-ws", "--workspace",
                            help="where to build workspace")
        parser.add_argument("-v", "--verbosity",
                            help="increase output verbosity",
                            type=int, choices=[0, 1, 2], default=0)
        fe = FastEval(parser.parse_args())
#+end_src

#+RESULTS:

** Dépendances

#+begin_src python :tangle fast_eval/util.py :noweb yes
  # Pour lecture de dossiers/fichiers
  import os
  import sys
  import csv
  import json
  import shlex
  # Pour affichage de dict
  import pprint
  # Pour décomprésser
  import shutil
  # Pour Exécution de programmes
  import subprocess

  from colored import fg, bg, attr
  # Helpers
  <<list_files>>
  <<search_files>>
  <<choice_str>>
#+end_src

** TODO Class
*** Init
   Initialization :
#+begin_src python :tangle fast_eval/util.py :noweb yes
  class FastEval:
      """
      @brief Simple tool to provide automation to assessment processes.
      @details Provide tools to build, compile and evaluatue a suitable
      workspace with a specific working folder for each submitted
      project from a single compressed archive.

      """
      def __init__(self, args):
          "docstring"
          self.ecolor = bg('indian_red_1a') + fg('white')
          #self.ecolor = fg('red_3a')
          #self.wcolor = bg('orange_1') + fg('white')
          self.wcolor = fg('orange_1')
          #self.icolor = bg('deep_sky_blue_2') + fg('white')
          #self.icolor = fg('medium_turquoise') + attr('bold')
          self.icolor = fg('light_sea_green') + attr('bold')
          self.rcolor = attr('reset')
          if args.workspace:
              self.workspace_path = os.path.abspath(os.path.expanduser(args.workspace))
          else:
              self.workspace_path = os.path.join(os.getcwd(), 'submissions')
          print(f'Using  {self.info_str(self.workspace_path)} as workspace. {self.info_str("✓")}')

          self.archive_path = os.path.expanduser(args.archive_path)
          if not os.path.exists(self.archive_path):
              print('Given  {}'
                    ' does not exist, exiting...'.format(self.erro_str(self.archive_path)),
                    file=sys.stderr)
              sys.exit()

          self.verbosity = args.verbosity
          config_path = os.path.expanduser(args.config)
          assert os.path.isfile(config_path), "{} is not a file.".format(self.erro_str(config_path))

          with open(config_path, 'r') as fp:
              config = json.load(fp)
          print(f'Loaded {self.info_str(config_path)} savefile. {self.info_str("✓")}')
          self.required_files = config['required_files']

          if len(config['reference_folder']) > 0:
              self.ref_path = os.path.expanduser(config['reference_folder'])
              if not os.path.isdir(self.ref_path):
                  print('Given  {}'
                    ' does not exist, exiting...'.format(self.erro_str(self.ref_path)),
                    file=sys.stderr)
                  sys.exit()
              print(f'Using  {self.info_str(self.ref_path)} as reference folder. {self.info_str("✓")}')
          else:
              self.ref_path = None
              print('Not using ref folder')

          if 'compilation_commands' in config:
              self.comp_cmd = config['compilation_commands']
          else:
              self.comp_cmd = []
          if 'execution_commands' in config:
              self.exec_cmd = config['execution_commands']
          else:
              self.exec_cmd = []
          if 'cleanup' in config:
              self.cleanup_cmd = config['cleanup']
          else:
              self.cleanup_cmd = []

          self.submissions = {}
          # Chargement de la config
          self.load_data()
          # Si c'est le premier passage, il faut lancer la preparation
          if self.pass_count == 0:
              shutil.unpack_archive(self.archive_path, self.workspace_path)
              submissions = self.clean_dirs()
              print('Processing {} projects...\n'.format(len(submissions)))
              self.submissions = {key: dict(value, **{'step' : '0_prep', 'steps': {'0_prep' : {},
                                                                                   '1_comp' : {},
                                                                                   '2_exec' : {},
                                                                                   '3_eval' : {}}}) for key, value in submissions.items()}
              self.extract_dirs()
              self.copy_ref()
              print('\n')
              self.prep_step()
          else:
              print('Processing {} projects...\n'.format(len(self.submissions)))
              self.check_prep()

          self.print_step_errors('0_prep')
          self.write_data()
          self.exte_step(self.comp_cmd, step='1_comp', label='Compiling')
          self.print_step_errors('1_comp')
          self.write_data()
          #self.exte_step(self.exec_cmd, step='2_exec', label='Executing')
          self.print_step_errors('2_exec')
          self.write_data()
          self.export()

      <<load_data>>
      <<write_data>>
      <<clean_dirs>>
      <<extract_dirs>>
      <<copy_ref>>
      <<prep_step>>
      <<check_prep>>
      <<exte_step>>
      <<cleanup>>
      <<export>>
      <<next_step>>
      <<erro_str>>
      <<warn_str>>
      <<info_str>>
      <<print_step_errors>>


#+end_src

*** Print Helpers
#+name: choice_str
#+begin_src python
  def choice_str(choices, target=''):
      res = '. ' + str(target) + '\n' + '│\n'
      for choice in choices[:-1]:
        res = res + '├── ' + str(choice) + '\n'
      res = res + '└── ' + choices[-1]
      return res
#+end_src

#+name: warn_str
#+begin_src python
  def warn_str(self, msg):
      return self.wcolor + str(msg) + self.rcolor
#+end_src

#+name: erro_str
#+begin_src python
  def erro_str(self, msg):
      return self.ecolor + str(msg) + self.rcolor
#+end_src

#+name: info_str
#+begin_src python
  def info_str(self, msg):
      return self.icolor + str(msg) + self.rcolor
#+end_src

#+name: print_step_errors
#+begin_src python
  def print_step_errors(self, step):
      to_print = [sub for sub in self.submissions if self.submissions[sub]['step'] == step]
      if self.verbosity >= 1 and len(to_print) > 0:
          print(f"Fail list : {to_print}\n")
      if self.verbosity > 1:
          for s in to_print:
              print(f'{s}\'s errors : \n {self.submissions[s]["steps"][step]}')
      print("\n")
#+end_src

*** Json data files
#+name: load_data
#+begin_src python
  def load_data(self):
      data_file = os.path.join(self.workspace_path, 'data.json')
      #data = load_json(data_file)
      try:
          with open(data_file, 'r') as fp:
              data = json.load(fp)


          self.pass_count = data['pass_count'] + 1
          self.submissions = data['submissions']
          print(f'Loaded {self.info_str(data_file)} savefile. {self.info_str("✓")}\n')
      except FileNotFoundError:
          print(f'Using  {self.info_str(data_file)} savefile. {self.info_str("✓")}\n')
          self.pass_count = 0
#+end_src

#+name: write_data
#+begin_src python
    def write_data(self):
        data_file = os.path.join(self.workspace_path, 'data.json')
        try:
            with open(data_file, 'w') as fp:
                json.dump({'pass_count': self.pass_count,
                           'submissions': self.submissions},
                          fp, sort_keys=True, indent=4, ensure_ascii=False)
            print(f'Wrote  {self.info_str(data_file)} savefile. {self.info_str("✓")}')
        except:
            print('Error while writing : \n => {}\n'.format(data_file),
                  file=sys.stderr)

#+end_src
*** Préparation
#+name: clean_dirs
#+begin_src python
  def clean_dirs(self):
      submissions = {o[:-32]:{"path": os.path.join(self.workspace_path, o)} for o in os.listdir(self.workspace_path)
                     if os.path.isdir(os.path.join(self.workspace_path, o))}
      for sub in submissions.values():
          if not os.path.exists(sub["path"][:-32]):
              shutil.move(sub['path'], sub['path'][:-32])
          if 'assignsubmission_file' in sub ['path']:
              sub['path'] = sub['path'][:-32]
      return submissions
#+end_src

#+name: extract_dirs
#+begin_src python
  def extract_dirs(self):
      for sub in self.submissions:
          raw_dir = os.path.join(self.submissions[sub]['path'], 'raw')
          os.mkdir(raw_dir)
          for o in os.listdir(self.submissions[sub]['path']):
              shutil.move(os.path.join(self.submissions[sub]['path'],o), raw_dir)
          files = [os.path.join(raw_dir, o) for o in os.listdir(raw_dir)]
          for f in files:
              try:
                  shutil.unpack_archive(f, raw_dir)
                  os.remove(f)
              except shutil.ReadError:
                  print('Unpack ' + self.warn_str(f) + ' failed.')

#+end_src

#+name: copy_ref
#+begin_src python
  def copy_ref(self):
      if self.ref_path is not None:
          for sub in self.submissions:
              shutil.copytree(self.ref_path, os.path.join(self.submissions[sub]['path'], 'eval'))

#+end_src

#+name: prep_step
#+begin_src python
  def prep_step(self):
      to_prep = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
      print('Preparing  {} projects...'.format(len(to_prep)))
      for sub in to_prep:
          raw_dir = os.path.join(self.submissions[sub]['path'], 'raw')
          eval_dir = os.path.join(self.submissions[sub]['path'], 'eval')

          if not os.path.exists(eval_dir):
              os.mkdir(eval_dir)

          missing_files = []

          # Search every required files one by one
          for f in self.required_files:
              # List cadidates for searched file
              student_code = search_files(f, raw_dir)
              # Filter files in a "__MACOS" directory
              student_code = [s for s in student_code if '__MACOS' not in s]
              if len(student_code) == 1:
                  shutil.copyfile(student_code[0], os.path.join(eval_dir, f))
              elif len(student_code) == 0:
                  missing_files.append(f)
              else:
                  msg = 'You need to manually copy one of those files'
                  msg = msg + choice_str(student_code, f)
                  self.submissions[sub]['steps']['0_prep']['msg'] = msg

          # Update missing files if needed
          if missing_files:
              if 'missing_files' not in self.submissions[sub]['steps']['0_prep']:
                  self.submissions[sub]['steps']['0_prep']['missing_files'] = missing_files
              else:
                  self.submissions[sub]['steps']['0_prep']['missing_files'].extend(missing_files)
          else:
              self.submissions[sub]['step'] = '1_comp'

      to_prep = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
      if len(to_prep) == 0:
          print(f'           0 fails. {self.info_str("✓")}')
      else:
          print('           ' + self.erro_str('{} fails.'.format(len(to_prep))) + '\n')
#+end_src
#+name: search_files
#+begin_src python
  def search_files(name, d='.'):
      return [os.path.join(root, f) for root, _, files in os.walk(d) for f in files if f == name]
#+end_src

#+name: check_prep
#+begin_src python
  def check_prep(self):
      to_check = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
      print('Checking   {} projects...'.format(len(to_check)))
      for sub in to_check:
          eval_dir = os.path.join(self.submissions[sub]['path'], 'eval')
          eval_files = [f for root, dirs, files in os.walk(eval_dir) for f in files]


          missing_files = [f for f in self.required_files if f not in eval_files]
          # Update missing files if needed
          if missing_files:
              self.submissions[sub]['steps']['0_prep']['missing_files'] = missing_files
          else:
              self.submissions[sub]['step'] = '1_comp'

      to_check = [sub for sub in self.submissions if self.submissions[sub]['step'] == '0_prep']
      print('           ' + self.erro_str('{} fails.'.format(len(to_check))) + '\n')
#+end_src
*** Compilation

#+name: next_step
#+begin_src python
  def next_step(self, step):
      if step == '0_prep':
          return '1_comp'
      elif step == '1_comp':
          return '2_exec'
      elif step == '2_exec':
          return '3_eval'
      else:
          return 'done'
#+end_src

#+name: exte_step
#+begin_src python
  def exte_step(self, cmd, step='1_comp', label='Compiling', timeout=10):
      to_exec = [sub for sub in self.submissions if self.submissions[sub]['step'] == step]
      print('{}  {} projects...'.format(label, len(to_exec)))
      root_dir = os.getcwd()
      for sub in to_exec:
          os.chdir(os.path.join(self.submissions[sub]['path'], 'eval'))
          comp_ok = True
          timeout_raised = False
          for c in cmd:
              try:
                  completed_process = subprocess.run([c], capture_output=True, text=True, shell=True, timeout=timeout)
                  if completed_process.returncode == 1:
                      comp_ok=False
                  cond = [len(completed_process.stderr) > 0, len(completed_process.stdout)]
                  if any(cond) and c not in self.submissions[sub]['steps'][step]:
                      self.submissions[sub]['steps'][step][c] = {}
                  if cond[0]:
                      self.submissions[sub]['steps'][step][c]['stderr'] = completed_process.stderr.split('\n')
                  if cond[1]:
                      out = completed_process.stdout.split('\n')
                      if len(out) > 20:
                          out = out[:10] + ['.'] + ['truncated by fast-eval'] + ['.'] + out[-10:]
                      self.submissions[sub]['steps'][step][c]['stdout'] = out
              except Exception as e:
                  comp_ok=False
                  if type(e) is subprocess.TimeoutExpired:
                      self.submissions[sub]['steps'][step][c] = 'timeout'

          if comp_ok:
              self.submissions[sub]['step'] = self.next_step(step)
      os.chdir(root_dir)
      to_exec = [sub for sub in self.submissions if self.submissions[sub]['step'] == step]
      if len(to_exec) == 0:
          print(f'           0 fails. {self.info_str("✓")}')
      else:
          print('           ' + self.erro_str('{} fails.'.format(len(to_exec))) + '\n')
      self.cleanup()

#+end_src

*** Cleanup

#+name: cleanup
#+begin_src python
  def cleanup(self):
      for c in self.cleanup_cmd:
          completed_process = subprocess.run(shlex.split(c))
          if completed_process.returncode == 0:
              print(f'Cleanup : {c} {self.info_str("✓")}')
          else:
              print(f'Cleanup : {c} {self.erro_str("❌")}')
#+end_src
*** Export vers org-mode

#+name: export
#+begin_src python
  def export(self):
      outpath = os.path.join(self.workspace_path, 'readme.org')
      with open(outpath, 'w') as f:
          f.write("#+title: Rapport d'évaluation\n")
          for s in self.submissions:
            step = self.submissions[s]['step']
            f.write(f'** {s}\n')

            # Section erreur prep
            if step == '0_prep':
                f.write(f'*** Erreurs de préparation\n')
                for k, v in self.submissions[s]['steps']['0_prep'].items():
                    f.write(f'{k} :\n')
                    for i in v:
                        f.write(f' - {i}\n')
            # Section erreur comp
            if step != '0_prep':
                ce = self.submissions[s]['steps']['1_comp']
                if ce:
                    f.write(f'*** Erreurs de compilation\n')
                for k, v in ce.items():
                    f.write(f'#+begin_src bash\n')
                    f.write(f'{k}\n')
                    f.write('#+end_src\n')
                    f.write('\n#+name: stderror\n')
                    f.write(f'#+begin_example\n')
                    for line in v['stderr']:
                        f.write(f'{line}\n')
                    f.write('\n#+end_example\n')

            # Section avec code rendu
            if step != '0_prep':
                f.write(f'*** code\n')
                for sf in self.required_files:
                    f.write(f'**** {sf}\n')
                    # Détermination du langage
                    l = os.path.splitext(sf)[-1][1:]
                    if l == 'py':
                      l = python
                    if l == 'sh':
                      l = bash
                    # Copie du code de l'étudiant
                    f.write(f'#+begin_src {l}\n')
                    with open(os.path.join(self.submissions[s]['path'], 'eval', sf), 'r') as cf:
                          f.write(cf.read())
                    f.write('\n#+end_src\n')

            # Section retour exécution
            if step != '0_prep' and step != '1_comp':
                e = self.submissions[s]['steps']['2_exec']
                if e:
                    f.write(f"*** Retours d'éxécution\n")
                for k, v in e.items():
                    f.write(f'#+begin_src bash\n')
                    f.write(f'{k}\n')
                    f.write('#+end_src\n')
                    if 'stderr' in v:
                        f.write('\n#+name: stderror\n')
                        f.write(f'#+begin_example\n')
                        for line in v['stderr']:
                            f.write(f'{line}\n')
                        f.write('#+end_example\n')
                    if 'stdout' in v:
                        f.write('\n#+name: stdout\n')
                        f.write(f'#+begin_example\n')
                        for line in v['stdout']:
                            f.write(f'{line}\n')
                        f.write('#+end_example\n')

#+end_src
* Déploiement vers Pypi

#+BEGIN_SRC bash :results output
rm -rf dist/
python setup.py sdist
#+END_SRC

#+RESULTS[8b5455aa48284574821df6568e30b543c07617d9]:
#+begin_example
running sdist
running egg_info
writing fast_eval.egg-info/PKG-INFO
writing dependency_links to fast_eval.egg-info/dependency_links.txt
writing entry points to fast_eval.egg-info/entry_points.txt
writing requirements to fast_eval.egg-info/requires.txt
writing top-level names to fast_eval.egg-info/top_level.txt
reading manifest file 'fast_eval.egg-info/SOURCES.txt'
writing manifest file 'fast_eval.egg-info/SOURCES.txt'
running check
creating fast-eval-0.2.7
creating fast-eval-0.2.7/fast_eval
creating fast-eval-0.2.7/fast_eval.egg-info
copying files to fast-eval-0.2.7...
copying setup.py -> fast-eval-0.2.7
copying fast_eval/__init__.py -> fast-eval-0.2.7/fast_eval
copying fast_eval/__main__.py -> fast-eval-0.2.7/fast_eval
copying fast_eval/util.py -> fast-eval-0.2.7/fast_eval
copying fast_eval.egg-info/PKG-INFO -> fast-eval-0.2.7/fast_eval.egg-info
copying fast_eval.egg-info/SOURCES.txt -> fast-eval-0.2.7/fast_eval.egg-info
copying fast_eval.egg-info/dependency_links.txt -> fast-eval-0.2.7/fast_eval.egg-info
copying fast_eval.egg-info/entry_points.txt -> fast-eval-0.2.7/fast_eval.egg-info
copying fast_eval.egg-info/requires.txt -> fast-eval-0.2.7/fast_eval.egg-info
copying fast_eval.egg-info/top_level.txt -> fast-eval-0.2.7/fast_eval.egg-info
Writing fast-eval-0.2.7/setup.cfg
creating dist
Creating tar archive
removing 'fast-eval-0.2.7' (and everything under it)
#+end_example

#+BEGIN_SRC bash
  twine upload dist/*
#+END_SRC

#+RESULTS:
* Github Pages

#+BEGIN_SRC bash
mkdir docs
#+END_SRC

#+BEGIN_SRC yaml :tangle docs/_config.yml
theme: jekyll-theme-architect
#+END_SRC

#+BEGIN_SRC bash
cp readme.md docs/index.md
#+END_SRC

#+RESULTS:
